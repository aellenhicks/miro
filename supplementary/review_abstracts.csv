Title,Abstract,Reviewer 1,Reviewer 3,Reviewer 4,Verdict
Semantic Model for Legal Resources: Annotation and Reasoning over Normative Provisions,"A Semantic Web approach for an advanced access to legislative documents is presented in terms of a model of normativeprovisions and related axioms. In particular, relations between provisions are identified and modeled by introducing patterns able to describe Hohfeldian legal fundamental relations. Moreover, a query-based approach able to deal with relations between provision specific instances is described. Examples of semantic annotation of legal textual resources using RDF/OWL standards, as well as advanced access and reasoning facilities over provisions using SPARQL, are shown. The main benefit of the approach is represented by the ability to keep the complexity of the problem within a description logic computational tractability.",0,0,0,E
LOTED2: An Ontology of European Public Procurement Notices,"Copyright and Moral Rights for the articles on this site are retained by the individual authors and/or other copyright owners. For more information on Open Research Online's data policy on reuse of materials please consult the policies page. Abstract. This paper describes the construction of the LOTED2 ontology for the representation of European public procurement notices. LOTED2follows initiatives around the creation of linked data-compliant representations of information regarding tender notices in Europe, but focusing on placing such representations within their legal context. It is therefore considered a legal ontology, as it supports the identification of legal concepts and more generally, legal reasoning. Unlike many other legal ontologies however, LOTED2 is designed to support the creation of Semantic Web applications. The methodology applied for building LOTED2 therefore seeks to find a compromise between the accurate representation of legal concepts and the usability of the ontology as a knowledge model for Semantic Web applications, while creating connections to other relevant ontologies in the domain.",1,1,1,I
"PPROC, an Ontology for Transparency in Public Procurement","Public procurement or tendering refers to the process followed by public authorities for the procurement of goods and services. In most developed countries, the law requires public authorities to provide online information to ensure competitive tendering as far as possible, for which the adequate announcement of tenders is an essential requirement. In addition, transparency laws being proposed in such countries are making the monitoring of public contracts by citizens a fundamental right. This paper describes the PPROC ontology, which has been developed to give support to both processes, publication and accountability, by semantically describing public procurement processes and contracts. The PPROC ontology is extensive, since it covers not only the usual data about the tender, its objectives, deadlines and awardees, but also details of the whole process, from the initial contract publication to its termination. This makes it possible to use the ontology for both open data publication purposes and for the overall management of the public contract procurement process.",1,1,1,I
Overview of the MPEG-21 Media Contract Ontology,"The MPEG-21 Media Contract Ontology (MCO), a part of the standard ISO/IEC 21000, is an ontology to represent contracts dealing with rights on multimedia assets and intellectual property protected content in general. A core model provides the elements to describe the permissions, obligations and prohibitions exchanged in the clauses of a contract. Specific vocabulary is defined in a model extension to represent the most common rights and constraints in the audiovisual context. Design principles, a methodology and a comparative analysis are given, as well as the practical guidelines to use the standard. A thorough description of the contract creation workflow from an original contract is given, including a sample contract text, the RDF version, the detailed mapping of the most relevant clauses and the reconstructed version. A set of MCO-related tools is described, including (i) the reference software to create and edit MCO contracts; (ii) modules to identify, store, search, validate and deliver MCO contracts and (iii) a tool to convert between the akin Contract Expression Language (CEL) contracts and the MCO contracts and (iv) the actual use of MCO in the Rightsdraw family of services.",1,1,0,I
The Document Components Ontology (DoCO),"The description of document layers, as well as of the document discourse (e.g. the scientific discourse in scholarly articles) in machine-readable forms is crucial in facilitating semantic publishing and overall comprehension of documents by both users and machines. In this paper we introduce DoCO, the Document Components Ontology, i.e., an OWL 2 DL ontology that provides a general-purpose structured vocabulary of document elements to describe document parts in RDF. In addition to the formal description of the ontology, its utility in practice is showcased through several in-house solutions and other works of the Semantic Publishing community that rely on DoCO to annotate and retrieve document components of scholarly articles. ",1,1,1,I
Time Ontology Extended for Non-Gregorian Calendar Applications,"We have extended OWL-Time to support the encoding of temporal position in a range of reference systems, in addition to the Gregorian calendar and conventional clock. Two alternative implementations are provided: as a pure extension or OWL-Time, or as a replacement, both of which preserve the same representation for the cases originally supported by OWL-Time. The combination of the generalized temporal position encoding and the temporal interval topology from OWL-Time support a range of applications in a variety of cultural and technical settings. These are illustrated with examples involving non-Gregorian calendars, Unix-time, and geologic time using both chronometric and stratigraphic timescales.",0,0,0,E
PAROLE/SIMPLE ‘lemon’ ontology and lexicons,"The PAROLE/SIMPLE 'Lemon "" Ontology and Lexicon are the OWL/RDF version of the PAROLE/SIMPLE lexicons (defined during the PAROLE (LE2-4017) and SIMPLE (LE4-8346) IV FP EU projects) once mapped onto Lemon model and LexInfo ontology. Original PAROLE/SIMPLE lexicons contain morphological, syntactic and semantic information , organized according to a common model and to common linguistic specifications for 12 European languages. The data set we describe includes the PAROLE/SIMPLEmodel mapped to Lemon and LexInfo ontology and the Spanish & Catalan lexicons. All data are published in the Data Hub and are distributed under CC Attribution 3.0 Unported license. The Spanish lexicon contains 199466 triples and 7572 lexical entries fully annotated with syntactic and semantic information. The Catalan lexicon contains 343714 triples and 20545 lexical entries annotated with syntactic information half of which are also annotated with semantic information. In this paper we describe the resulting data, the mapping process and the benefits obtained. We demonstrate that the Linked Open Data principles prove essential for datasets such as original PAROLE/SIMPLE lexicons where harmonization and interoperability was crucial. The resulting data is lighter and better suited for exploitation. In addition , it easies further extensions and links to external resources such as WordNet, lemonUby, DBpedia etc.",0,0,0,E
"lemonUby - a large, interlinked, syntactically-rich lexical resource for ontologies","We introduce lemonUby, a new lexical resource integrated in the Semantic Web which is the result of converting data extracted from the existing large-scale linked lexical resource UBY to the lemon lexicon model. The following data from UBY were converted: WordNet, FrameNet, VerbNet, English and German Wiktionary, the English and German entries of Omega-Wiki, as well as links between pairs of these lexicons at the word sense level (links between VerbNet and FrameNet, VerbNet and WordNet, WordNet and FrameNet, WordNet and Wiktionary, WordNet and German OmegaWiki). We linked lemonUby to other lexical resources and linguistic terminology repositories in the Linguistic Linked Open Data cloud and outline possible applications of this new dataset.",0,0,0,E
OLiA – Ontologies of Linguistic Annotation," This paper describes the Ontologies of Linguistic Annotation (OLiA) as one of the data sets currently available as part of Linguistic Linked Open Data (LLOD) cloud. Within the LLOD cloud, the OLiA ontologies serve as a reference hub for annotation terminology for linguistic phenomena on a great band-width of languages, they have been used to facilitate interoperability and information integration of linguistic annotations in corpora, NLP pipelines, and lexical-semantic resources and mediate their linking with multiple community-maintained terminology repositories.",1,1,0,I
Using ontologies to model human navigation behavior in information networks: A study based on Wikipedia,"The need to examine the behavior of different user groups is a fundamental requirement when building information systems. In this paper, we present Ontology-based Decentralized Search (OBDS), a novel method to model the navigation behavior of users equipped with different types of background knowledge. Ontology-based Decentralized Search combines ontologies and decentralized search, an established method for navigation in social networks, to model navigation behavior in information networks. The method uses ontologies as an explicit representation of background knowledge to inform the navigation process and guide it towards navigation targets. By using different ontologies, users equipped with different types of background knowledge can be represented. We demonstrate our method using four biomedical ontologies and their associated Wikipedia articles. We compare our simulation results with base line approaches and with results obtained from a user study and find that our method produces click paths that have properties similar to those originating from human navigators. The results suggest that our method can be used to model human navigation behavior in systems that are based on information networks such as Wikipedia. This paper makes the following contributions: (i) To the best of our knowledge, this is the first work to demonstrate the utility of ontologies in modeling human navigation and (ii) it yields new insights and understanding about the mechanisms of human navigation in information networks.",0,0,0,E
"DBpedia - A Large-scale, Multilingual Knowledge Base Extracted from Wikipedia","The DBpedia community project extracts structured, multilingual knowledge from Wikipedia and makes it freely available using Semantic Web and Linked Data standards. The extracted knowledge, comprising more than 1.8 billion facts, is structured according to an ontology maintained by the community. The knowledge is obtained from different Wikipedia language editions, thus covering more than 100 languages, and mapped to the community ontology. The resulting data sets are linked to more than 30 other data sets in the Linked Open Data (LOD) cloud. The DBpedia project was started in 2006 and has meanwhile attracted large interest in research and practice. Being a central part of the LOD cloud, it serves as a connection hub for other data sets. For the research community, DBpedia provides a testbed serving real world data spanning many domains and languages. Due to the continuous growth of Wikipedia, DBpedia also provides an increasing added value for data acquisition, re-use and integration tasks within organisations. In this system report, we give an overview over the DBpedia community project, including its architecture, technical implementation, maintenance, internationalisation, usage statistics and showcase some popular DBpedia applications.",0,0,0,E
LC3: A spatio-temporal and semantic model for knowledge discovery from geospatial datasets,"There is a need for decision-makers to be provided with both an overview of existing knowledge, and information which is as complete and up-to-date as possible on changes in certain features of the biosphere. Another objective is to bring together all the many attempts which have been made over the years at various levels (international, Community, national and regional) to obtain more information on the environment and the way it is changing. As a result, remote sensing tools monitor large amount of land cover informationg enabling study of dynamic processes. However the size of the dataset requires new tools to identify pattern and extract knowledge. We propose a model to discover knowledge on parcel data allowing analysis of dynamic geospatial phenomena using time, spatial and thematic data. The model is called Land Cover Change Continuum (LC3) and is able to track the evolution of spatial entities along time. Based on semantic web technologies, the model allows users to specify and to query spatio-temporal information based on semantic definitions. The semantic of spatial relationships are of interest to qualify filiation relationships. The result of this process permit to identify evolutive patterns as a basis for studying the dynamics of the geospatial environment. To this end, we use CORINE datasets to study changes in a specific part of France. In our approach, we consider entities as having several representations during their lifecycle. Each representation includes identity, spatial and descriptive properties that evolve over time.",0,0,0,E
Exposing INSPIRE on the Semantic Web,"The INSPIRE Directive by the European Commission sets the legal and technical foundations towards interoperable Spatial Data Infrastructures (SDIs) across Europe. EU member states are already providing such services for several geospatial data themes (e.g., transportation networks, administrative units). Unfortunately, the INSPIRE ecosystem has been largely disjoint from the Semantic Web, without any means to repurpose existing SDIs as high-quality data sources, and thus multiply their value through interlinking, reasoning and inferencing. In this paper, we introduce a methodology that can assist stakeholders in exposing INSPIRE-aligned SDIs on the Semantic Web according to the recent GeoSPARQL standard. We develop methods for discovering INSPIRE data through a virtual SPARQL endpoint over existing INSPIRE catalogue services. Further, we implement a suite of tools for automatically transforming INSPIRE data and metadata into RDF triples with geometries. The compiled geographic and thematic information can then be loaded into semantic repositories for querying or interlinked with other data. Our open-source solutions essentially repurpose existing INSPIRE SDIs, so as to promote uptake and facilitate their reuse in practice. Finally, as a case study, we report our experience in validating this approach on a real-world SDI with publicly available data for Greece in order to expose its contents through (Geo)SPARQL endpoints.",0,0,0,E
Engineering ontology-based access to real-world data sources,"The preparation of existing real-world datasets for publication as high-quality semantic web data is a complex task that requires the concerted execution of a variety of processing steps using a range of different tools. Faced with both changing input data and evolving requirements on the produced output, we face a significant engineering task for schema and data transformation. We argue that to achieve a robust and flexible transformation process, a high-level declarative description is needed, that can be used to drive the entire tool chain. We have implemented this idea for the deployment of ontology-based data access (OBDA) solutions, where semantically annotated views that integrate multiple data sources on different formats are created, based on an ontology and a collection of mappings. For illustration, we show how a single declarative description helps to orchestrate a complete tool chain, beginning with the download of data sets, and through to the installation of the data sets for a variety of tools including data and query transformation processes and reasoning services. We base our case study on several publicly available tabular and relational datasets concerning the operations of the petroleum industry in Norway. We include a discussion of the relative performance of the used tools on our case study, and an overview of lessons learnt for practical deployment of OBDA on real-world datasets.",0,0,0,E
The Data Mining OPtimization Ontology,"The Data Mining OPtimization Ontology (DMOP) has been developed to support informed decision-making at various choice points of the data mining process. The ontology can be used by data miners and deployed in ontology-driven information systems. The primary purpose for which DMOP has been developed is the automation of algorithm and model selection through semantic meta-mining that makes use of an ontology-based meta-analysis of complete data mining processes in view of extracting patterns associated with mining performance. To this end, DMOP contains detailed descriptions of data mining tasks (e.g., learning, feature selection), data, algorithms, hypotheses such as mined models or patterns, and workflows. A development methodology was used for DMOP, including items such as competency questions and foundational ontology reuse. Several non-trivial modeling problems were encountered and due to the complexity of the data mining details, the ontology requires the use of the OWL 2 DL profile. DMOP was successfully evaluated for semantic meta-mining and used in constructing the Intelligent Discovery Assistant, deployed at the popular data mining environment RapidMiner.",1,1,1,I
Using a suite of ontologies for preserving workflow-centric research objects,"Scientific workflows are a popular mechanism for specifying and automating data-driven in silico experiments. A significant aspect of their value lies in their potential to be reused. Once shared, workflows become useful building blocks that can be combined or modified for developing new experiments. However, previous studies have shown that storing workflow specifications alone is not sufficient to ensure that they can be successfully reused, without being able to understand what the workflows aim to achieve or to re-enact them. To gain an understanding of the workflow, and how it may be used and repurposed for their needs, scientists require access to additional resources such as annotations describing the workflow, datasets used and produced by the workflow, and provenance traces recording workflow executions. In this article, we present a novel approach to the preservation of scientific workflows through the application of research objects-aggregations of data and metadata that enrich the workflow specifications. Our approach is realised as a suite of ontologies that support the creation of workflow-centric research objects. Their design was guided by requirements elicited from previous empirical analyses of workflow decay and repair. The ontologies developed make use of and extend existing well known ontologies, namely the Object Reuse and Exchange (ORE) vocabulary, the Annotation Ontology (AO) and the W3C PROV ontology (PROVO). We illustrate the application of the ontologies for building Workflow Research Objects with a case-study that investigates Huntington's disease, performed in collaboration with a team from the Leiden University Medial Centre (HG-LUMC). Finally we present a number of tools developed for creating and managing Workflow-Centric Research Objects.",0,0,0,E
WSMO-Lite and hRESTS: Lightweight semantic annotations for Web services and RESTful APIs,"Available online xxxx Keywords: WSMO-Lite SAWSDL Web services RESTful services a b s t r a c t Service-oriented computing has brought special attention to service description, especially in connection with semantic technologies. The expected proliferation of publicly accessible services can benefit greatly from tool support and automation, both of which are the focus of Semantic Web Service (SWS) frameworks that especially address service discovery, composition and execution. As the first SWS standard, in 2007 the World Wide Web Consortium produced a lightweight bottom-up specification called SAWSDL for adding semantic annotations to WSDL service descriptions. Building on SAWSDL, this article presents WSMO-Lite, a lightweight ontology of Web service semantics that distinguishes four semantic aspects of services: function, behavior, information model, and nonfunctional properties, which together form a basis for semantic automation. With the WSMO-Lite ontology, SAWSDL descriptions enable semantic automation beyond simple input/output matchmaking that is supported by SAWSDL itself. Further, to broaden the reach of WSMO-Lite and SAWSDL tools to the increasingly common RESTful services, the article adds hRESTS and MicroWSMO, two HTML microformats that mirror WSDL and SAWSDL in the documentation of RESTful services, enabling combining RESTful services with WSDL-based ones in a single semantic framework. To demonstrate the feasibility and versatility of this approach, the article presents common algorithms for Web service discovery and composition adapted to WSMO-Lite.",1,1,0,I
Mímir: An open-source semantic search framework for interactive information seeking and discovery,"Semantic search is gradually establishing itself as the next generation search paradigm, which meets better a wider range of information needs, as compared to traditional full-text search. At the same time, however, expanding search towards document structure and external, formal knowledge sources (e.g. LOD resources) remains challenging, especially with respect to efficiency, usability, and scalability. This paper introduces Mímir – an open-source framework for integrated semantic search over text, document structure , linguistic annotations, and formal semantic knowledge. Mímir supports complex structural queries, as well as basic keyword search. Exploratory search and sense-making are supported through information visualisation interfaces, such as co-occurrence matrices and term clouds. There is also an interactive retrieval interface, where users can save, refine, and analyse the results of a semantic search over time. The more well-studied precision-oriented information seeking searches are also well supported. The generic and extensible nature of the Mímir platform is demonstrated through three different, real-world applications , one of which required indexing and search over tens of millions of documents and fifty to hundred times as many semantic annotations. Scaling up to over 150 million documents was also accomplished, via index federation and cloud-based deployment.",0,0,0,E
Ontology-based representation and analysis of host-Brucella interactions,"Background: Biomedical ontologies are representations of classes of entities in the biomedical domain and how these classes are related in computer-and human-interpretable formats. Ontologies support data standardization and exchange and provide a basis for computer-assisted automated reasoning. IDOBRU is an ontology in the domain of Brucella and brucellosis. Brucella is a Gram-negative intracellular bacterium that causes brucellosis, the most common zoonotic disease in the world. In this study, IDOBRU is used as a platform to model and analyze how the hosts, especially host macrophages, interact with virulent Brucella strains or live attenuated Brucella vaccine strains. Such a study allows us to better integrate and understand intricate Brucella pathogenesis and host immunity mechanisms.",0,0,0,E
My Corporis Fabrica Embryo: An ontology-based 3D spatio-temporal modeling of human embryo development,"BACKGROUND:
Embryology is a complex morphologic discipline involving a set of entangled mechanisms, sometime difficult to understand and to visualize. Recent computer based techniques ranging from geometrical to physically based modeling are used to assist the visualization and the simulation of virtual humans for numerous domains such as surgical simulation and learning. On the other side, the ontology-based approach applied to knowledge representation is more and more successfully adopted in the life-science domains to formalize biological entities and phenomena, thanks to a declarative approach for expressing and reasoning over symbolic information. 3D models and ontologies are two complementary ways to describe biological entities that remain largely separated. Indeed, while many ontologies providing a unified formalization of anatomy and embryology exist, they remain only descriptive and make the access to anatomical content of complex 3D embryology models and simulations difficult.
RESULTS:
In this work, we present a novel ontology describing the development of the human embryology deforming 3D models. Beyond describing how organs and structures are composed, our ontology integrates a procedural description of their 3D representations, temporal deformation and relations with respect to their developments. We also created inferences rules to express complex connections between entities. It results in a unified description of both the knowledge of the organs deformation and their 3D representations enabling to visualize dynamically the embryo deformation during the Carnegie stages. Through a simplified ontology, containing representative entities which are linked to spatial position and temporal process information, we illustrate the added-value of such a declarative approach for interactive simulation and visualization of 3D embryos.
CONCLUSIONS:
Combining ontologies and 3D models enables a declarative description of different embryological models that capture the complexity of human developmental anatomy. Visualizing embryos with 3D geometric models and their animated deformations perhaps paves the way towards some kind of hypothesis-driven application. These can also be used to assist the learning process of this complex knowledge.",1,1,1,I
Improving the Sequence Ontology terminology for genomic variant annotation,"BACKGROUND:
The Genome Variant Format (GVF) uses the Sequence Ontology (SO) to enable detailed annotation of sequence variation. The annotation includes SO terms for the type of sequence alteration, the genomic features that are changed and the effect of the alteration. The SO maintains and updates the specification and provides the underlying ontologicial structure.
METHODS:
A requirements analysis was undertaken to gather terms missing in the SO release at the time, but needed to adequately describe the effects of sequence alteration on a set of variant genomic annotations. We have extended and remodeled the SO to include and define all terms that describe the effect of variation upon reference genomic features in the Ensembl variation databases.
RESULTS:
The new terminology was used to annotate the human reference genome with a set of variants from both COSMIC and dbSNP. A GVF file containing 170,853 sequence alterations was generated using the SO terminology to annotate the kinds of alteration, the effect of the alteration and the reference feature changed. There are four kinds of alteration and 24 kinds of effect seen in this dataset. (Ensembl Variation annotates 34 different SO consequence terms: http://www.ensembl.org/info/docs/variation/predicted_data.html).
CONCLUSIONS:
We explain the updates to the Sequence Ontology to describe the effect of variation on existing reference features. We have provided a set of annotations using this terminology, and the well defined GVF specification. We have also provided a provisional exploration of this large annotation dataset.",0,0,0,E
Development of an Ontology for Periodontitis,"In the clinical dentists and periodontal researchers' community, there is an obvious demand for a systems model capable of linking the clinical presentation of periodontitis to underlying molecular knowledge. A computer-readable representation of processes on disease development will give periodontal researchers opportunities to elucidate pathways and mechanisms of periodontitis. An ontology for periodontitis can be a model for integration of large variety of factors relating to a complex disease such as chronic inflammation in different organs accompanied by bone remodeling and immune system disorders, which has recently been referred to as osteoimmunology.
METHODS:
Terms characteristic of descriptions related to the onset and progression of periodontitis were manually extracted from 194 review articles and PubMed abstracts by experts in periodontology. We specified all the relations between the extracted terms and constructed them into an ontology for periodontitis. We also investigated matching between classes of our ontology and that of Gene Ontology Biological Process.
RESULTS:
We developed an ontology for periodontitis called Periodontitis-Ontology (PeriO). The pathological progression of periodontitis is caused by complex, multi-factor interrelationships. PeriO consists of all the required concepts to represent the pathological progression and clinical treatment of periodontitis. The pathological processes were formalized with reference to Basic Formal Ontology and Relation Ontology, which accounts for participants in the processes realized by biological objects such as molecules and cells. We investigated the peculiarity of biological processes observed in pathological progression and medical treatments for the disease in comparison with Gene Ontology Biological Process (GO-BP) annotations. The results indicated that peculiarities of Perio existed in 1) granularity and context dependency of both the conceptualizations, and 2) causality intrinsic to the pathological processes. PeriO defines more specific concepts than GO-BP, and thus can be added as descendants of GO-BP leaf nodes. PeriO defines causal relationships between the process concepts, which are not shown in GO-BP. The difference can be explained by the goal of conceptualization: PeriO focuses on mechanisms of the pathogenic progress, while GO-BP focuses on cataloguing all of the biological processes observed in experiments. The goal of conceptualization in PeriO may reflect the domain knowledge where a consequence in the causal relationships is a primary interest. We believe the peculiarities can be shared among other diseases when comparing processes in disease against GO-BP.
CONCLUSIONS:
This is the first open biomedical ontology of periodontitis capable of providing a foundation for an ontology-based model of aspects of molecular biology and pathological processes related to periodontitis, as well as its relations with systemic diseases. PeriO is available at http://bio-omix.tmd.ac.jp/periodontitis/.",1,1,1,I
Developing VISO: Vaccine Information Statement Ontology for patient education,"OBJECTIVE:
To construct a comprehensive vaccine information ontology that can support personal health information applications using patient-consumer lexicon, and lead to outcomes that can improve patient education.
METHODS:
The authors composed the Vaccine Information Statement Ontology (VISO) using the web ontology language (OWL). We started with 6 Vaccine Information Statement (VIS) documents collected from the Centers for Disease Control and Prevention (CDC) website. Important and relevant selections from the documents were recorded, and knowledge triples were derived. Based on the collection of knowledge triples, the meta-level formalization of the vaccine information domain was developed. Relevant instances and their relationships were created to represent vaccine domain knowledge.
RESULTS:
The initial iteration of the VISO was realized, based on the 6 Vaccine Information Statements and coded into OWL2 with Protégé. The ontology consisted of 132 concepts (classes and subclasses) with 33 types of relationships between the concepts. The total number of instances from classes totaled at 460, along with 429 knowledge triples in total. Semiotic-based metric scoring was applied to evaluate quality of the ontology.",1,1,1,I
Development and validation of a classification approach for extracting severity automatically from electronic health records,"Electronic Health Records (EHRs) contain a wealth of information useful for studying clinical phenotype-genotype relationships. Severity is important for distinguishing among phenotypes; however other severity indices classify patient-level severity (e.g., mild vs. acute dermatitis) rather than phenotype-level severity (e.g., acne vs. myocardial infarction). Phenotype-level severity is independent of the individual patient's state and is relative to other phenotypes. Further, phenotype-level severity does not change based on the individual patient. For example, acne is mild at the phenotype-level and relative to other phenotypes. Therefore, a given patient may have a severe form of acne (this is the patient-level severity), but this does not effect its overall designation as a mild phenotype at the phenotype-level.
METHODS:
We present a method for classifying severity at the phenotype-level that uses the Systemized Nomenclature of Medicine - Clinical Terms. Our method is called the Classification Approach for Extracting Severity Automatically from Electronic Health Records (CAESAR). CAESAR combines multiple severity measures - number of comorbidities, medications, procedures, cost, treatment time, and a proportional index term. CAESAR employs a random forest algorithm and these severity measures to discriminate between severe and mild phenotypes.
RESULTS:
Using a random forest algorithm and these severity measures as input, CAESAR differentiates between severe and mild phenotypes (sensitivity = 91.67, specificity = 77.78) when compared to a manually evaluated reference standard (k = 0.716).
CONCLUSIONS:
CAESAR enables researchers to measure phenotype severity from EHRs to identify phenotypes that are important for comparative effectiveness research.",0,0,0,E
Expanding the mammalian phenotype ontology to support automated exchange of high throughput mouse phenotyping data generated by large-scale mouse knockout screens,"Background A vast array of data is about to emerge from the large scale high-throughput mouse knockout phenotyping projects worldwide. It is critical that this information is captured in a standardized manner, made accessible, and is fully integrated with other phenotype data sets for comprehensive querying and analysis across all phenotype data types. The volume of data generated by the high-throughput phenotyping screens is expected to grow exponentially, thus, automated methods and standards to exchange phenotype data are required. Results The IMPC (International Mouse Phenotyping Consortium) is using the Mammalian Phenotype (MP) ontology in the automated annotation of phenodeviant data from high throughput phenotyping screens. 287 new term additions with additional hierarchy revisions were made in multiple branches of the MP ontology to accurately describe the results generated by these high throughput screens. Conclusions Because these large scale phenotyping data sets will be reported using the MP as the common data standard for annotation and data exchange, automated importation of these data to MGI (Mouse Genome Informatics) and other resources is possible without curatorial effort. Maximum biomedical value of these mutant mice will come from integrating primary high-throughput phenotyping data with secondary, comprehensive phenotypic analyses combined with published phenotype details on these and related mutants at MGI and other resources.",0,0,0,E
Development and application of an interaction network ontology for literature mining of vaccine-associated gene-gene interactions,"Background
Literature mining of gene-gene interactions has been enhanced by ontology-based name classifications. However, in biomedical literature mining, interaction keywords have not been carefully studied and used beyond a collection of keywords.

Methods
In this study, we report the development of a new Interaction Network Ontology (INO) that classifies >800 interaction keywords and incorporates interaction terms from the PSI Molecular Interactions (PSI-MI) and Gene Ontology (GO). Using INO-based literature mining results, a modified Fisher’s exact test was established to analyze significantly over- and under-represented enriched gene-gene interaction types within a specific area. Such a strategy was applied to study the vaccine-mediated gene-gene interactions using all PubMed abstracts. The Vaccine Ontology (VO) and INO were used to support the retrieval of vaccine terms and interaction keywords from the literature.

Results
INO is aligned with the Basic Formal Ontology (BFO) and imports terms from 10 other existing ontologies. Current INO includes 540 terms. In terms of interaction-related terms, INO imports and aligns PSI-MI and GO interaction terms and includes over 100 newly generated ontology terms with ‘INO_’ prefix. A new annotation property, ‘has literature mining keywords’, was generated to allow the listing of different keywords mapping to the interaction types in INO. Using all PubMed documents published as of 12/31/2013, approximately 266,000 vaccine-associated documents were identified, and a total of 6,116 gene-pairs were associated with at least one INO term. Out of 78 INO interaction terms associated with at least five gene-pairs of the vaccine-associated sub-network, 14 terms were significantly over-represented (i.e., more frequently used) and 17 under-represented based on our modified Fisher’s exact test. These over-represented and under-represented terms share some common top-level terms but are distinct at the bottom levels of the INO hierarchy. The analysis of these interaction types and their associated gene-gene pairs uncovered many scientific insights.

Conclusions
INO provides a novel approach for defining hierarchical interaction types and related keywords for literature mining. The ontology-based literature mining, in combination with an INO-based statistical interaction enrichment test, provides a new platform for efficient mining and analysis of topic-specific gene interaction networks.",1,1,1,I
The cellular microscopy phenotype ontology,"Background
Phenotypic data derived from high content screening is currently annotated using free-text, thus preventing the integration of independent datasets, including those generated in different biological domains, such as cell lines, mouse and human tissues.

Description
We present the Cellular Microscopy Phenotype Ontology (CMPO), a species neutral ontology for describing phenotypic observations relating to the whole cell, cellular components, cellular processes and cell populations. CMPO is compatible with related ontology efforts, allowing for future cross-species integration of phenotypic data. CMPO was developed following a curator-driven approach where phenotype data were annotated by expert biologists following the Entity-Quality (EQ) pattern. These EQs were subsequently transformed into new CMPO terms following an established post composition process.

Conclusion
CMPO is currently being utilized to annotate phenotypes associated with high content screening datasets stored in several image repositories including the Image Data Repository (IDR), MitoSys project database and the Cellular Phenotype Database to facilitate data browsing and discoverability.",1,1,1,I
The Non-Coding RNA Ontology (NCRO): a comprehensive resource for the unification of non-coding RNA biology,"In recent years, sequencing technologies have enabled the identification of a wide range of non-coding RNAs (ncRNAs). Unfortunately, annotation and integration of ncRNA data has lagged behind their identification. Given the large quantity of information being obtained in this area, there emerges an urgent need to integrate what is being discovered by a broad range of relevant communities. To this end, the Non-Coding RNA Ontology (NCRO) is being developed to provide a systematically structured and precisely defined controlled vocabulary for the domain of ncRNAs, thereby facilitating the discovery, curation, analysis, exchange, and reasoning of data about structures of ncRNAs, their molecular and cellular functions, and their impacts upon phenotypes. The goal of NCRO is to serve as a common resource for annotations of diverse research in a way that will significantly enhance integrative and comparative analysis of the myriad resources currently housed in disparate sources. It is our belief that the NCRO ontology can perform an important role in the comprehensive unification of ncRNA biology and, indeed, fill a critical gap in both the Open Biological and Biomedical Ontologies (OBO) Library and the National Center for Biomedical Ontology (NCBO) BioPortal. Our initial focus is on the ontological representation of small regulatory ncRNAs, which we see as the first step in providing a resource for the annotation of data about all forms of ncRNAs. The NCRO ontology is free and open to all users, accessible at: http://purl.obolibrary.org/obo/ncro.owl.",1,1,1,I
OBIB-a novel ontology for biobanking,"Background
Biobanking necessitates extensive integration of data to allow data analysis and specimen sharing. Ontologies have been demonstrated to be a promising approach in fostering better semantic integration of biobank-related data. Hitherto no ontology provided the coverage needed to capture a broad spectrum of biobank user scenarios.

Methods
Based in the principles laid out by the Open Biological and Biomedical Ontologies Foundry two biobanking ontologies have been developed. These two ontologies were merged using a modular approach consistent with the initial development principles. The merging was facilitated by the fact that both ontologies use the same Upper Ontology and re-use classes from a similar set of pre-existing ontologies.

Results
Based on the two previous ontologies the Ontology for Biobanking (http://purl.obolibrary.org/obo/obib.owl) was created. Due to the fact that there was no overlap between the two source ontologies the coverage of the resulting ontology is significantly larger than of the two source ontologies. The ontology is successfully used in managing biobank information of the Penn Medicine BioBank.

Conclusions
Sharing development principles and Upper Ontologies facilitates subsequent merging of ontologies to achieve a broader coverage.",1,1,0,I
VICO: Ontology-based representation and integrative analysis of Vaccination Informed Consent forms,"Background

Although signing a vaccination (or immunization) informed consent form is not a federal requirement in the US and Canada, such a practice is required by many states and pharmacies. The content and structures of these informed consent forms vary, which makes it hard to compare and analyze without standardization. To facilitate vaccination informed consent data standardization and integration, it is important to examine various vaccination informed consent forms, patient answers, and consent results. In this study, we report a Vaccination Informed Consent Ontology (VICO) that extends the Informed Consent Ontology and integrates related OBO foundry ontologies, such as the Vaccine Ontology, with a focus on vaccination screening questionnaire in the vaccination informed consent domain.

Results

Current VICO contains 993 terms, including 248 VICO specific terms and 709 terms imported from 17 OBO Foundry ontologies. VICO ontologically represents and integrates 12 vaccination informed consent forms from the Walgreens, Costco pharmacies, Rite AID, University of Maryland College Park, and the government of Manitoba, Canada. VICO extends Informed Consent Ontology (ICO) with vaccination screening questionnaires and questions. Our use cases and examples demonstrate five usages of VICO. First, VICO provides standard, robust and consistent representation and organization of the knowledge in different vaccination informed consent forms, questionnaires, and questions. Second, VICO integrates prior knowledge, e.g., the knowledge of vaccine contraindications imported from the Vaccine Ontology (VO). Third, VICO helps manage the complexity of the domain knowledge using logically defined ontological hierarchies and axioms. VICO glues multiple schemas that represent complex vaccination informed consent contents defined in different organizations. Fourth, VICO supports efficient query and comparison, e.g., through the Description Language (DL)-Query and SPARQL. Fifth, VICO helps discover new knowledge. For instance, by integrating the prior knowledge imported from the VO with a user’s answer to informed consent questions (e.g., allergic reaction question) for a specific vaccination, we can infer whether or not the patient can be vaccinated with the vaccine.

Conclusions

The Vaccination Informed Consent Ontology (VICO) represents entities related to vaccination informed consents with a special focus on vaccination informed consent forms, and questionnaires and questions in the forms. Our use cases and examples demonstrated how VICO could support a platform for vaccination informed consent data standardization, data integration, and data queries.",1,0,1,I
Extending gene ontology in the context of extracellular RNA and vesicle communication,"Background

To address the lack of standard terminology to describe extracellular RNA (exRNA) data/metadata, we have launched an inter-community effort to extend the Gene Ontology (GO) with subcellular structure concepts relevant to the exRNA domain. By extending GO in this manner, the exRNA data/metadata will be more easily annotated and queried because it will be based on a shared set of terms and relationships relevant to extracellular research.

Methods

By following a consensus-building process, we have worked with several academic societies/consortia, including ERCC, ISEV, and ASEMV, to identify and approve a set of exRNA and extracellular vesicle-related terms and relationships that have been incorporated into GO. In addition, we have initiated an ongoing process of extractions of gene product annotations associated with these terms from Vesiclepedia and ExoCarta, conversion of the extracted annotations to Gene Association File (GAF) format for batch submission to GO, and curation of the submitted annotations by the GO Consortium. As a use case, we have incorporated some of the GO terms into annotations of samples from the exRNA Atlas and implemented a faceted search interface based on such annotations.

Results

We have added 7 new terms and modified 9 existing terms (along with their synonyms and relationships) to GO. Additionally, 18,695 unique coding gene products (mRNAs and proteins) and 963 unique non-coding gene products (ncRNAs) which are associated with the terms: “extracellular vesicle”, “extracellular exosome”, “apoptotic body”, and “microvesicle” were extracted from ExoCarta and Vesiclepedia. These annotations are currently being processed for submission to GO.

Conclusions

As an inter-community effort, we have made a substantial update to GO in the exRNA context. We have also demonstrated the utility of some of the new GO terms for sample annotation and metadata search.",0,0,0,E
"MicrO: an ontology of phenotypic and metabolic characters, assays, and culture media found in prokaryotic taxonomic descriptions","BACKGROUND:
MicrO is an ontology of microbiological terms, including prokaryotic qualities and processes, material entities (such as cell components), chemical entities (such as microbiological culture media and medium ingredients), and assays. The ontology was built to support the ongoing development of a natural language processing algorithm, MicroPIE (or, Microbial Phenomics Information Extractor). During the MicroPIE design process, we realized there was a need for a prokaryotic ontology which would capture the evolutionary diversity of phenotypes and metabolic processes across the tree of life, capture the diversity of synonyms and information contained in the taxonomic literature, and relate microbiological entities and processes to terms in a large number of other ontologies, most particularly the Gene Ontology (GO), the Phenotypic Quality Ontology (PATO), and the Chemical Entities of Biological Interest (ChEBI). We thus constructed MicrO to be rich in logical axioms and synonyms gathered from the taxonomic literature.
RESULTS:
MicrO currently has ~14550 classes (~2550 of which are new, the remainder being microbiologically-relevant classes imported from other ontologies), connected by ~24,130 logical axioms (5,446 of which are new), and is available at (http://purl.obolibrary.org/obo/MicrO.owl) and on the project website at https://github.com/carrineblank/MicrO. MicrO has been integrated into the OBO Foundry Library (http://www.obofoundry.org/ontology/micro.html), so that other ontologies can borrow and re-use classes. Term requests and user feedback can be made using MicrO's Issue Tracker in GitHub. We designed MicrO such that it can support the ongoing and future development of algorithms that can leverage the controlled vocabulary and logical inference power provided by the ontology.
CONCLUSIONS:
By connecting microbial classes with large numbers of chemical entities, material entities, biological processes, molecular functions, and qualities using a dense array of logical axioms, we intend MicrO to be a powerful new tool to increase the computing power of bioinformatics tools such as the automated text mining of prokaryotic taxonomic descriptions using natural language processing. We also intend MicrO to support the development of new bioinformatics tools that aim to develop new connections between microbial phenotypes and genotypes (i.e., the gene content in genomes). Future ontology development will include incorporation of pathogenic phenotypes and prokaryotic habitats.",1,1,1,I
Representing vision and blindness,"BACKGROUND:
There have been relatively few attempts to represent vision or blindness ontologically. This is unsurprising as the related phenomena of sight and blindness are difficult to represent ontologically for a variety of reasons. Blindness has escaped ontological capture at least in part because: blindness or the employment of the term 'blindness' seems to vary from context to context, blindness can present in a myriad of types and degrees, and there is no precedent for representing complex phenomena such as blindness.
METHODS:
We explore current attempts to represent vision or blindness, and show how these attempts fail at representing subtypes of blindness (viz., color blindness, flash blindness, and inattentional blindness). We examine the results found through a review of current attempts and identify where they have failed.
RESULTS:
By analyzing our test cases of different types of blindness along with the strengths and weaknesses of previous attempts, we have identified the general features of blindness and vision. We propose an ontological solution to represent vision and blindness, which capitalizes on resources afforded to one who utilizes the Basic Formal Ontology as an upper-level ontology.
CONCLUSIONS:
The solution we propose here involves specifying the trigger conditions of a disposition as well as the processes that realize that disposition. Once these are specified we can characterize vision as a function that is realized by certain (in this case) biological processes under a range of triggering conditions. When the range of conditions under which the processes can be realized are reduced beyond a certain threshold, we are able to say that blindness is present. We characterize vision as a function that is realized as a seeing process and blindness as a reduction in the conditions under which the sight function is realized. This solution is desirable because it leverages current features of a major upper-level ontology, accurately captures the phenomenon of blindness, and can be implemented in many domain-specific ontologies.",1,1,1,I
An accurate and precise representation of drug ingredients,"BACKGROUND:
In previous work, we built the Drug Ontology (DrOn) to support comparative effectiveness research use cases. Here, we have updated our representation of ingredients to include both active ingredients (and their strengths) and excipients. Our update had three primary lines of work: 1) analysing and extracting excipients, 2) analysing and extracting strength information for active ingredients, and 3) representing the binding of active ingredients to cytochrome P450 isoenzymes as substrates and inhibitors of those enzymes.
METHODS:
To properly differentiate between excipients and active ingredients, we conducted an ontological analysis of the roles that various ingredients, including excipients, have in drug products. We used the value specification model of the Ontology for Biomedical Investigations to represent strengths of active ingredients and then analyzed RxNorm to extract excipient and strength information and modeled them according to the results of our analysis. We also analyzed and defined dispositions of molecules used in aggregate as active ingredients to bind cytochrome P450 isoenzymes.
RESULTS:
Our analysis of excipients led to 17 new classes representing the various roles that excipients can bear. We then extracted excipients from RxNorm and added them to DrOn for branded drugs. We found excipients for 5,743 branded drugs, covering ~27% of the 21,191 branded drugs in DrOn. Our analysis of active ingredients resulted in another new class, active ingredient role. We also extracted strengths for all types of tablets, capsules, and caplets, resulting in strengths for 5,782 drug forms, covering ~41% of the 14,035 total drug forms and accounting for ~97 % of the 5,970 tablets, capsules, and caplets in DrOn. We represented binding-as-substrate and binding-as-inhibitor dispositions to two cytochrome P450 (CYP) isoenzymes (CYP2C19 and CYP2D6) and linked these dispositions to 65 compounds. It is now possible to query DrOn automatically for all drug products that contain active ingredients whose molecular grains inhibit or are metabolized by a particular CYP isoenzyme. DrOn is open source and is available at http://purl.obolibrary.org/obo/dron.owl.",0,0,0,E
Towards exergaming commons: composing the exergame ontology for publishing open game data,"BACKGROUND:
It has been shown that exergames have multiple benefits for physical, mental and cognitive health. Only recently, however, researchers have started considering them as health monitoring tools, through collection and analysis of game metrics data. In light of this and initiatives like the Quantified Self, there is an emerging need to open the data produced by health games and their associated metrics in order for them to be evaluated by the research community in an attempt to quantify their potential health, cognitive and physiological benefits.
METHODS:
We have developed an ontology that describes exergames using the Web Ontology Language (OWL); it is available at http://purl.org/net/exergame/ns#. After an investigation of key components of exergames, relevant ontologies were incorporated, while necessary classes and properties were defined to model these components. A JavaScript framework was also developed in order to apply the ontology to online exergames. Finally, a SPARQL Endpoint is provided to enable open data access to potential clients through the web.
RESULTS:
Exergame components include details for players, game sessions, as well as, data produced during these game-playing sessions. The description of the game includes elements such as goals, game controllers and presentation hardware used; what is more, concepts from already existing ontologies are reused/repurposed. Game sessions include information related to the player, the date and venue where the game was played, as well as, the results/scores that were produced/achieved. These games are subsequently played by 14 users in multiple game sessions and the results derived from these sessions are published in a triplestore as open data.
CONCLUSIONS:
We model concepts related to exergames by providing a standardized structure for reference and comparison. This is the first work that publishes data from actual exergame sessions on the web, facilitating the integration and analysis of the data, while allowing open data access through the web in an effort to enable the concept of Open Trials for Active and Healthy Ageing.",1,1,1,I
An ontology for major histocompatibility restriction,"MHC molecules are a highly diverse family of proteins that play a key role in cellular immune recognition. Over time, different techniques and terminologies have been developed to identify the specific type(s) of MHC molecule involved in a specific immune recognition context. No consistent nomenclature exists across different vertebrate species.
PURPOSE:
To correctly represent MHC related data in The Immune Epitope Database (IEDB), we built upon a previously established MHC ontology and created an ontology to represent MHC molecules as they relate to immunological experiments.
DESCRIPTION:
This ontology models MHC protein chains from 16 species, deals with different approaches used to identify MHC, such as direct sequencing verses serotyping, relates engineered MHC molecules to naturally occurring ones, connects genetic loci, alleles, protein chains and multi-chain proteins, and establishes evidence codes for MHC restriction. Where available, this work is based on existing ontologies from the OBO foundry.
CONCLUSIONS:
Overall, representing MHC molecules provides a challenging and practically important test case for ontology building, and could serve as an example of how to integrate other ontology building efforts into web resources.",1,1,1,I
